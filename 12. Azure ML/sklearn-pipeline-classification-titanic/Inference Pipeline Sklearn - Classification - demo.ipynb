{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Pipeline using Scikit-learn models\n",
    "<br />\n",
    "1. build sklearn pipeline locally<br />\n",
    "\n",
    "A). **preprocessor**<br />   \n",
    "   it will perform preprocessing of numeric cat cols<br />\n",
    "   \n",
    "*numeric* : imputation, scaling<br />\n",
    "\n",
    "*categoric* : imputation, one-hot-encoding<br />\n",
    "      \n",
    "B). **classifier**<br />\n",
    "    LogisticRegression Classifier<br />\n",
    "    \n",
    "<br />\n",
    "2. tune the pipeline model<br />\n",
    "\n",
    "<br />\n",
    "3. Register the artifacts<br />\n",
    "train/test data<br />\n",
    "pipeline model<br />\n",
    "\n",
    "<br />\n",
    "4. deploy inference-ml-pipeline as an endpoint<br />\n",
    "\n",
    "<br />\n",
    "5. prediction using the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 100\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  prepare raw data\n",
    "This data will be used for training/testing the ml pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_predict = 'survived'\n",
    "col_primary_identifer = \"user_id\"\n",
    "\n",
    "COLLIST_META = [col_primary_identifer]\n",
    "COLLIST_FEATURE = ['age', 'fare', 'embarked', 'sex', 'pclass']\n",
    "COLLIST_ALL     = [col_to_predict] + COLLIST_META + COLLIST_FEATURE\n",
    "COLLIST_NUMERIC = ['age', 'fare']\n",
    "COLLIST_CATEGORICAL = ['embarked', 'sex', 'pclass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_TRAIN_PATH = \"./data_dir/train_data.csv\"\n",
    "RAW_TEST_PATH  = \"./data_dir/test_data.csv\"\n",
    "RAW_VAL_PATH   = \"./data_dir/val_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run only during first run\n",
    "#!mkdir ./data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_data: (1309, 7), train: (850, 7), test: (328, 7), val: (131, 5)\n",
      "(1309, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>user_id</th>\n",
       "      <th>age</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "      <th>sex</th>\n",
       "      <th>pclass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>user_0</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>211.3375</td>\n",
       "      <td>S</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>user_1</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>S</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived user_id      age      fare embarked     sex  pclass\n",
       "0         1  user_0  29.0000  211.3375        S  female       1\n",
       "1         1  user_1   0.9167  151.5500        S    male       1"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_url = ('https://raw.githubusercontent.com/amueller/'\n",
    "               'scipy-2017-sklearn/091d371/notebooks/datasets/titanic3.csv')\n",
    "data = pd.read_csv(titanic_url)\n",
    "data[\"user_id\"] = [\"user_\"+str(i) for i in range(len(data))]\n",
    "\n",
    "X = data[COLLIST_ALL].copy()\n",
    "\n",
    "train_data, test_data, val_data = np.split(X.sample(frac=1, random_state=SEED), [int(.65*len(X)), int(.9*len(X))])\n",
    "val_data = val_data[COLLIST_FEATURE]\n",
    "\n",
    "# save the data\n",
    "train_data[COLLIST_NUMERIC].to_csv(path_or_buf=RAW_TRAIN_PATH, index=False, header=None)\n",
    "test_data[COLLIST_NUMERIC].to_csv(path_or_buf=RAW_TEST_PATH, index=False, header=None)\n",
    "val_data[COLLIST_NUMERIC].to_csv(path_or_buf=RAW_VAL_PATH, index=False, header=None)\n",
    "\n",
    "print(\"raw_data: {}, train: {}, test: {}, val: {}\".format(X.shape, train_data.shape, test_data.shape, val_data.shape))\n",
    "X.head(2)\n",
    "\n",
    "print(X.shape)\n",
    "X.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data.csv  train_data.csv  val_data.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. build sklearn pipeline locally\n",
    "1. Pipeline setup\n",
    "2. Train\n",
    "3. Tune\n",
    "4. Persist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data[COLLIST_FEATURE]\n",
    "X_test  = test_data[COLLIST_FEATURE]\n",
    "y_train = train_data[col_to_predict]\n",
    "y_test  = test_data[col_to_predict]\n",
    "\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['age', 'fare', 'embarked', 'sex', 'pclass'], dtype=object)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[COLLIST_FEATURE].columns.values\n",
    "#['age', 'fare', 'embarked', 'sex', 'pclass']\n",
    "#  0,        1,       2,       3,      4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('preprocessor', ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n",
       "         transformer_weights=None,\n",
       "         transformers=[('num', Pipeline(memory=None,\n",
       "     steps=[('imputer', SimpleImputer(copy=True, fill_value=None, missing_values=nan,\n",
       "       strategy='median', verbo...enalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We create the preprocessing pipelines for both numeric and categorical data.\n",
    "numeric_features = [0, 1]\n",
    "categorical_features = [2, 3, 4]\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', LogisticRegression(solver='lbfgs'))])\n",
    "\n",
    "clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model score: 0.799\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "payload    :  [[8.0, 36.75, 'S', 'male', 2]]\n",
      "prediction :  [0]\n"
     ]
    }
   ],
   "source": [
    "payload = val_data[0:1].values.tolist()\n",
    "pred = clf.predict(payload)\n",
    "\n",
    "print(\"payload    : \", payload)\n",
    "print(\"prediction : \", pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best logistic regression from grid search: 0.799\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'preprocessor__num__imputer__strategy': ['mean', 'median'],\n",
    "    'classifier__C': [0.1, 1.0, 10, 100],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=10, iid=False)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print((\"best logistic regression from grid search: %.3f\"\n",
    "       % grid_search.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('preprocessor', ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n",
       "         transformer_weights=None,\n",
       "         transformers=[('num', Pipeline(memory=None,\n",
       "     steps=[('imputer', SimpleImputer(copy=True, fill_value=None, missing_values=nan, strategy='mean',\n",
       "       verbose...enalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_main = grid_search.best_estimator_\n",
    "model_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_main.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## persist the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sklearn_clf_pipeline_titanic_v1.pkl']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model_main, 'sklearn_clf_pipeline_titanic_v1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Register the artifacts\n",
    "1. input and output datasets\n",
    "2. pipeline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bais-ml\n",
      "ml-resource-group\n",
      "eastus\n",
      "328d2afe-2a26-4e47-8e3b-db00b6ada105\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input and output datasets\n",
    "\n",
    "Here, you will register the data used to create the model in your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0\n",
       "0  0\n",
       "1  0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# persisting the training data : features/label in .csv format\n",
    "temp_X_train = pd.DataFrame(X_train)\n",
    "temp_y_train = pd.DataFrame(y_train)\n",
    "\n",
    "temp_X_train.to_csv(path_or_buf=\"features.csv\", index=False)\n",
    "temp_y_train.to_csv(path_or_buf=\"labels.csv\", index=False)\n",
    "\n",
    "pd.read_csv(\"./features.csv\").head(2)\n",
    "pd.read_csv(\"./labels.csv\").head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading an estimated of 2 files\n",
      "Uploading ./features.csv\n",
      "Uploading ./labels.csv\n",
      "Uploaded ./labels.csv, 1 files out of an estimated total of 2\n",
      "Uploaded ./features.csv, 2 files out of an estimated total of 2\n",
      "Uploaded 2 files\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from azureml.core import Dataset\n",
    "\n",
    "#np.savetxt('features.csv', X_train, delimiter=',')\n",
    "#np.savetxt('labels.csv', y_train, delimiter=',')\n",
    "\n",
    "datastore = ws.get_default_datastore()\n",
    "datastore.upload_files(files=['./features.csv', './labels.csv'],\n",
    "                       target_path='ds_sklearn_clf_titanic/',\n",
    "                       overwrite=True)\n",
    "\n",
    "input_dataset_clf = Dataset.Tabular.from_delimited_files(path=[(datastore, 'ds_sklearn_clf_titanic/features.csv')])\n",
    "output_dataset_clf = Dataset.Tabular.from_delimited_files(path=[(datastore, 'ds_sklearn_clf_titanic/labels.csv')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls -al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model titanic-sklearn-model\n",
      "Name: titanic-sklearn-model\n",
      "Version: 8\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "\n",
    "from azureml.core import Model\n",
    "from azureml.core.resource_configuration import ResourceConfiguration\n",
    "\n",
    "\n",
    "model = Model.register(workspace=ws,\n",
    "                       model_name='titanic-sklearn-model',                # Name of the registered model in your workspace.\n",
    "                       model_path='sklearn_clf_pipeline_titanic_v1.pkl',  # Local file to upload and register as a model.\n",
    "                       model_framework=Model.Framework.SCIKITLEARN,  # Framework used to create the model.\n",
    "                       model_framework_version=sklearn.__version__,  # Version of scikit-learn used to create the model.\n",
    "                       sample_input_dataset=input_dataset_clf,\n",
    "                       sample_output_dataset=output_dataset_clf,\n",
    "                       resource_configuration=ResourceConfiguration(cpu=1, memory_in_gb=0.5),\n",
    "                       description='titanic clf model to predict survival.',\n",
    "                       tags={'area': 'titanic', 'type': 'clf'})\n",
    "\n",
    "print('Name:', model.name)\n",
    "print('Version:', model.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Deploy model\n",
    "\n",
    "Deploy your model as a web service using [Model.deploy()](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.model?view=azure-ml-py#deploy-workspace--name--models--inference-config--deployment-config-none--deployment-target-none-). Web services take one or more models, load them in an environment, and run them on one of several supported deployment targets. For more information on all your options when deploying models, see the [next steps](#Next-steps) section at the end of this notebook.\n",
    "\n",
    "For this example, we will deploy your scikit-learn model to an Azure Container Instance (ACI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running...........................................\n",
      "Succeeded\n",
      "ACI service creation operation finished, operation \"Succeeded\"\n"
     ]
    }
   ],
   "source": [
    "service_name = 'service-titanic-sklearn-clf-v7'\n",
    "\n",
    "service = Model.deploy(ws, service_name, [model], overwrite=True)\n",
    "service.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check the logs uncomment the below line\n",
    "\n",
    "#print(service.get_logs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_payload :  {\"data\": [[8.0, 36.75, \"S\", \"male\", 2], [37.0, 29.7, \"C\", \"male\", 1]], \"method\": \"predict\"}\n",
      "prediction    :  [0 1]\n"
     ]
    }
   ],
   "source": [
    "payload = val_data[0:2].values.tolist()\n",
    "\n",
    "input_payload = json.dumps({\n",
    "    'data': payload,\n",
    "    'method': 'predict'  # If you have a classification model, you can get probabilities by changing this to 'predict_proba'.\n",
    "})\n",
    "\n",
    "parsed_data = json.loads(input_payload)[\"data\"]\n",
    "pred = model_main.predict(parsed_data)\n",
    "\n",
    "print(\"input_payload : \", input_payload)\n",
    "print(\"prediction    : \", pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# terminate end-point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "service.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
